\documentclass[11pt,
  titlepage=false,
  abstract=on,
  %parskip=half,      % enable if you want paragraphs separated by vertical spacing instead of indents
]{scrreprt}

% Style settings
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage[acronym]{glossaries}
\addtokomafont{disposition}{\rmfamily}
\renewcommand*\thesection{\arabic{section}}

% Useful packages for complex content:
\usepackage{amsmath,amsfonts,amssymb} % typesetting math
%\usepackage{siunitx}                 % typesetting SI-units and formatted numbers
\usepackage{listings}                % typesetting source code
\usepackage{booktabs,multirow}        % utils for complex/beautiful tables
%\usepackage{subcaption}              % placing multiple subfigures in a figure
\usepackage{graphicx}                % including external image files
\usepackage{tikz}                    % drawing figures within LaTeX
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}

% Bibliography, referencing, and indexing
\usepackage{csquotes}                 % typesetting \enquote{text in quotes} correctly
\usepackage[backend=biber,
style=alphabetic,
minalphanames=3, maxalphanames=4,
maxbibnames=20]{biblatex} % to generate the bibliography
\addbibresource{report.bib}           % name of the bib-file

\makeglossaries

% Useful utils:
%\usepackage{todonotes}               % add ToDo markers (\todo{...}, \todo[inline]{...})
\usepackage[hidelinks]{hyperref}      % clickable links (but hide color frames around links)
%\usepackage{cleveref}                % named references (\Cref{sec:introduction}, ...)

% Your own macros:
%\newcommand{\mynewmacro}[1]{my content with one input parameter: #1}


\begin{document}

%--- FRONT MATTER --------------------------------------------------------------

\title{Evaluation of modular multiplication methods used in lattice-based cryptography on FPGA/ASIC}
\author{Teodora-Alexandra Alexandrescu}
\date{Introduction to Scientific Working 2024/25\bigskip\bigskip}
\publishers{\normalsize
  Supervisor:
  Aikata Aikata
  \medskip\par
  Institute of Applied Information Processing and Communications\\
  Graz University of Technology
}

\maketitle


\begin{abstract}\noindent
Lattice-based cryptography is the backbone for an entire class of systems offering post-quantum security.
Many of these cryptosystems require rigorous mathematical approaches for laying the foundation of security, especially in the case of 
post-quantum computing or fully-homomorphic encryption schemes. A well-known NP-hard problem that is fundamental for both previously mentioned algorithms
is Ring Learning with Errors, which uses polynomial arithmetic. In several cases, if these polynomials are small and a naive computational method is used,
the performance costs will also be minimal. However, the higher the polynomial degree, the higher the need for optimization becomes.\\
To solve this problem, the concept of modular multiplication has been applied, where each coefficient in the polynomial result needs to be reduced modulo a quotient $q$ in order to reduce the coefficients. Thus, modular multiplication is the most fundamental resource in cryptographic schemes. This arithmetic operation, 
if performed efficiently, can bring a significant impact over the performance of lattice-based cryptographic implementations.\\
In this work, we address the problem of efficiency in terms of area and power consumption on digital platforms such as FPGA/ASIC
and analyze three well-known modular multipliers for evaluation: Montgomery Reduction, Barrett's Reduction and Shift \& Add. Furthermore, we seek to distinguish between two novel usecases of lattice-based cryptography,
namely Post-Quantum Cryptography and Fully-Homomorphic Encryption, based on how they utilize the NP-Hard Ring Learning with Errors problem.

% Provide an abstract of your report (at most $\frac{1}{2}$ page for this report, typically 1 to 3 paragraphs).

% The abstract usually consists of two main parts: a motivational background and your contribution.
% Start with a few sentences of general introduction and background information to motivate your main research question/challenge.
% Then, summarize what your paper contributes and describe its (potential) impact.
% This includes a very short summary of all your important results and core performance numbers that characterize your approach/attack/countermeasure/implementation.
% Finally, summarize any key conclusions and calls to action that you have, e.g., apply the idea more broadly, get rid of some technology, find a countermeasure, or similar.


\paragraph*{Keywords:}
Lattice-based Cryptography $\cdot$
Post-Quantum Cryptography$\cdot$
Fully-Homomorphic Encryption$\cdot$
Modular Multipliers $\cdot$
FPGA $\cdot$
ASIC
\end{abstract}

\clearpage


%--- INTRODUCTION --------------------------------------------------------------

\section{Introduction}
\label{sec:introduction}
% To further explore the intricacies of these cryptographic schemes, 
% In asymmetric cryptography, ensuring that the private key remains secret under any circumstances is crucial. (\texttt{not entirely sure about this, 
% there's no punchline and it doesn't catch the attention of the reader. for sure an actual scientific question must be stated.})

Lattice-based constructions have gained a lot of attention in recent times due to their complexity, a characteristic that correlates with enhanced security.
As the need for secure and lightweight lattice-based algorithms increases, their complexity is also an aspect that requires intensive study, especially
in the context of platforms like FPGA or ASIC, where area and power are limited resources. Cryptosystems based on lattices derive their hardness from the
Learning with Errors problem \cite{regev2010learning}, which is considered to be as hard as an NP-hard problem \cite{micciancio2013hardness}.

Fully-Homomorphic Encryption \& Post-Quantum Cryptography are at the forefront of cryptographic research. While FHE enables computation on encrypted data, PQC
addresses threats posed by quantum-counterparts. Both FHE and PQC schemes such as Genotype Imputation \cite{gursoy2022privacy} or NTTRU \cite{cryptoeprint:2019/040} are 
lattice-based due to how they use the (Ring -) Learning with Errors properties: the ring structure and polynomial representation, usually employed during encryption.

At the heart of these lattice-based constructions lies polynomial multiplication, which is the most computationally intensive operation in Ring Learning 
with Errors cryptosystems, with its time-complexity spanning $O(n^{2})$. To optimize this, Number Theoretic Transform (NTT) is applied to reduce the complexity to $O(nlogn)$.
One crucial operation during NTT is modular reduction, which ensures that the polynomial coefficients lie within manageable bounds while still keeping the initial 
security properties. However, the higher the security degree, the more convoluted these operations become and hence resources such as occupied area or power consumption 
need to be preserved.

For this reason, we aim to provide an evaluation of modular multiplication methods targeted on digital platforms, in order to be aware of possible bottlenecks 
or caveats during the implementation of such schemes. In this study, we will mainly discuss the Montgomery Reduction, Barrett Reduction and the Shift \& Add algorithm
in the context of lattice-based schemes on FPGA/ASIC.


Montgomery Reduction \cite{montgomery1985modular} is well-known for its efficiency in modular arithmetic due to replacing classical long division with bitwise operations, and by introducing the Montgomery form,
which is a precomputation step that facilitates a faster, non-traditional division operation.
On the other hand, Barrett's modular multiplication algorithm \cite{barrett1986implementing} is more robust to changes in modulus due to the introduction of a precomputation step which highly depends on the modulus and
its bit width. The inclusion of this step enables the algorithm to eliminate division in favor of less expensive multiplication, making it efficient.


A comparison between the Montgomery and Barrett modular multipliers has been carried out in \cite{kong2006comparison} on a Xilinx Virtex 2 FPGA platform using VHDL and a general modulus
(i.e. modulus without a specific structure, as seen in other studies).
The goal was to compare area and timing results when computing $C = A \cdot B \mod M$, where $A$, $B$, $C$, $M$ are all $n$-bit integers with lengths up to 32 bits.
For the Montgomery multipliers, four evaluation methods have been employed, and namely high-radix, separated/interleaved structure, trivial digit selection and quotient digit pipelining.
As for the Barrett multiplier, an improved separated method has been applied: $A \cdot B$ is precomputed by using the inverse of the modulus, $M^{-1}$.
Intermediate area results for high-radix separated/interleaved Montgomery reduction with radix (number base) $r = 2^{k}$ show that: for the interleaved  method, the area required in FPGA slices is higher
at smaller $k$-values (where k is the word length or bits per digit), but as $k$ increases, the results for both the interleaved and separated methods converge to the same point, as depicted in \hyperref[fig:interleaved-separated-area]{Figure~\ref{fig:interleaved-separated-area}}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/interleaved-separated-area.png}
  \caption{Area of Interleaved \& Separated Structures at Different Radices \cite{kong2006comparison}}
  \label{fig:interleaved-separated-area}
\end{figure}

The final results show that  for the current setup with a general modulus, the choice between Montgomery and Barrett highly depends on the word length. However, for word lengths up to 16 bits,
Barrett's reduction with the precomputed inverse of the modulus seems to be the appropriate choice.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/mod-mult-diff-wordlengths.png}
  \caption{Comparison of areas of modular multipliers at different wordlengths \cite{kong2006comparison}}
  \label{fig:mod-mult-diff-wordlengths}
\end{figure}

Shift \& Add is a standard multiplication method used in digital design. Its significance stems from the ability to reduce different algorithms (or particular parts of them) to simple operations
like bitshifts and additions. In \cite{sreehari2012application} it is proven that modular reduction speed is increased by $80$\% for a modified Barrett reduction algorithm when choosing Mersenne/ quasi-Mersenne numbers as moduli.
By exploiting their properties, the overall long-integer multiplication speed is increased when using simple shift and addition operations in the reduction step.  

A design space study of modular multipliers for Fully-Homomorphic Encryption (FHE) implemented on ASIC is presented in \cite{10129292}. The study explores both bit-parallel and bit-serial designs at the system and block levels. 
The results indicate that bit-parallel multipliers are more area- and power-efficient than bit-serial designs at the block level. Finally, the study concludes that Montgomery modular multiplication with a constrained modulus \footnote{it must follow specific properties, like M $\equiv 1 \mod 2N$, which restricts the number of possible moduli.}
offers the most efficient design in terms of area and power consumption, at both the system and block levels. This ensures optimal polynomial reduction for FHE on ASIC platforms.

At a larger scale, modular reduction is an important operation within NTT, which is widely used in lattice-based schemes. This transformation is usually employed on butterfly cores, which are mathematical structures that allow for
efficient modular polynomial operations by reducing the complexity of multiplying polynomials. By optimizing the modular reduction operation, we obtain more efficient polynomial multiplication. In \cite{9603378}, several 
improvements  for modular reduction in NTT for the post-quantum scheme Kyber \cite{bos2018crystals} are presented: pipelining, the KRED/K2RED functions \cite{longa2016speeding} and parallelization. The KRED optimization 
particularly contributes to the parallelization process due to the application of vector integer instructions in their implementation. Due to parallelism, pipelining is also improved. The results in \cite{9603378} state 
that the NTT core in this design achieves over a 44\% performance improvement, measured by the $A \times T$ metric (area $\times$ time), on an Artix-7 FPGA.

% \paragraph{Outline.}

%--- BACKGROUND ----------------------------------------------------------------

\section{Background}
\label{sec:background}

\subsection{Notation}
We use $\Lambda(a_1, a_2,..., a_n)$ to represent a lattice and its basis vector. Gaussian distributions are denoted with $\rho$, whereas Gaussian error distributions are marked by $\chi$.
Vector norms are represented by $|| \cdot ||$, and vector dot products by $\langle \ ,\ \rangle$.
We use $\mathbb{Z}_q$ to represent the set of integers modulo a quotient $q \geq 2$. We represent the set of $d$-dimensional vectors in $\mathbb{Z}^d_q$
with $d \geq 1$. The ring of all polynomials over a finite field of moduli is represented by $\mathbb{Z}_q / \langle x^d + 1\rangle$, where $x^d + 1$ is the modulus polynomial 
(i.e. all elements of the ring are reduced by this modulus). Mersenne primes are denoted as $M_p$.
\subsection{Brief description of Lattices}
In group theory, a lattice is a discrete subgroup of the additive group, or a free abelian group of dimension $m$ which spans the vector space $\mathbb{R}^m$ and has a periodic structure. 
Each point in the lattice is separated by a minimum distance, a property that underpins the periodic nature of the lattice structure. Coordinate-wise addition or subtraction of two lattice-points 
produces a new lattice-point.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/lattice-with-lattice-vectors.png}
  \caption{Lattice vectors in Euclidean plane \cite{latticesInPQC}}
  \label{fig:LatticeWithLatticeVectors}
\end{figure}

% \newpage

Mathematically, given $n$ linearly independent vectors $a_1, a_2,..., a_n \in \mathbb{R}^m$ as the basis, we define the lattice generated by them as follows \cite{latticesIntro2004}:
\begin{equation*}
  \Lambda(a_1, a_2,..., a_n) = \left\{\sum x_ia_i\ |\ x_i \in \mathbb{Z}\right\} 
\end{equation*}

In simpler terms, a lattice can be described as a vector space limited to integer multiples of its basis vectors.  A depiction of a bi-dimensional lattice can be observed in 
\hyperref[fig:LatticeWithLatticeVectors]{Figure~\ref{fig:LatticeWithLatticeVectors}}.

The minimum distance of a lattice in a norm $||\cdot||$ is the length of the shortest nonzero lattice vector. If the norm is unspecified, the default
value is the Euclidean norm.

The Gaussian distribution is an important metric in lattices, especially in lattice-based cryptography where computationally infeasible problems employ the distribution
to generate random noise in order to enhance hardness. For any lattice vector $\mathbf{x}$ with a center $\mathbf{c}$ in the discreteness of the lattice, and a scaling factor $s$,
we define the discrete Gaussian distribution as follows \cite{WCToAVGCase}:

\begin{equation*}
  \rho_{s,\mathbf{c}} = e^{-\pi {\| (\mathbf{x} - \mathbf{c}) / s\|}^2}
\end{equation*}

In other words, when the discrete Gaussian distribution is applied to a lattice vector near a given center, we can generate a noisy version of the center vector, which adds controlled randomness 
to the cryptographic application. Worst-case lattice problems such as Learning with Errors use this Gaussian distribution to create noise for the purpose of hardness.

% \cite{discreteGMeasuresLattices}
\subsection{Learning with Errors}
Modern-day experts base their encryption algorithms on hard mathematical problems such as the Learning with Errors problem to guarantee security within a certain standard.\\
After 2005, the Learning with Errors problem published by Oded Regev \cite{regev2010learning} became a standard for cryptographic constructions due the strict security requirements it enforces.
A study made by Micciancio \& Peikert proves its hardness to resemble the norms of approximate lattice problems in the worst case \cite{micciancio2013hardness}.
The problem states that a secret vector $\textbf{s}$ should be retrieved from a system of linear noisy equations, where the public vector $\textbf{a}$ is drawn uniformly at random
and the error term $\textit{e}$ is drawn from a Gaussian distribution. While it may be possible to recover the secret $\textbf{s}$
by performing Gaussian decomposition over the system of equations, the addition of the noise term makes it computationally infeasible to retrieve the
secret for appropriate parameters.
% We denote the Learning with Errors problem as following: $(\mathbf{a}, \langle \mathbf{a}, \mathbf{s} \rangle + e)$, where $\mathbf{a}$ is a vector
% drawn uniformly at random from $\mathbb{Z}^n_q$. The goal is to retrieve the secret vector $\mathbf{s} \in \mathbb{Z}^n_q$ given an error term
% $e$ drawn from an error.
\paragraph{Definition.} Given a lattice $\Lambda \subseteq \mathbb{R}$ with a vector basis $a_1, a_2,..., a_d$ drawn uniformly at random from $\mathbb{Z}^d_q$, a secret lattice vector $\mathbf{s}=\sum s_ia_i \in \mathbb{Z}^d_q$ 
and a noisy vector $\mathbf{e}$ drawn from a discrete Gaussian error distribution $\chi$ on $\mathbb{Z}_q$, the goal of the problem is to recover $\mathbf{s}$ from the approximated, noisy output 
$\mathbf{b} = (\mathbf{a}, \langle \mathbf{a}, \mathbf{s} \rangle + e)$. \cite{regev2010learning}\\

\[
  \begin{aligned}
  % \text{The Learning with Errors Problem:} \quad
a_1^{(1)} \cdot s_1 + \dots + a_d^{(1)} \cdot s_d &\approx b^{(1)} \pmod{q} \\
a_1^{(2)} \cdot s_1 + \dots + a_d^{(2)} \cdot s_d &\approx b^{(2)} \pmod{q} \\
&\vdots \\
a_1^{(n)} \cdot s_1 + \dots + a_d^{(n)} \cdot s_d &\approx b^{(n)} \pmod{q}
\label{The Learning with Errors Problem}
\end{aligned}
\]

\paragraph{Error distribution.}
The error distribution $\chi$ is chosen to be a normal (Gaussian) distribution. This is then rounded to the nearest integer of standard deviation ($\alpha q$, where $\alpha > 0$ is typically $\frac{1}{poly(d)}$)
and reduced modulo $q$, to ensure that the noise is small relative to the modulus.

\paragraph{Modulus.} The modulus $q$ is typically chosen to be a polynomial of the vector dimension $d$. The choice of a polynomial modulus significantly increases the size of the input, which further affects
the cryptographic application's efficiency in the favor of increasing the problem's hardness.

\paragraph{Number of equations.} Researchers have drawn the conclusion that the number of equations does not result in any changes in hardness - on the contrary, the hardness is independent of it.
This is due to a property discussed in \cite{regev2010learning}, where given a fixed polynomial number of equations,  it is possible to generate an arbitrarily large number of additional equations, 
which can be further utilized.\\

The parameters provided to Learning with Errors, either number of vectors or vector dimension, usually determine the complexity of the encryption operation \cite{micciancio2013hardness}, or more specifically, the key size. 
Cryptographic applications often need at least  $n$ vectors of $n$ dimensions, which lead to key sizes of $n^2$ \cite{regev2010learning}, which inherently means $O(n^{2})$ space complexity. This aspect can represent a challenge as the need for a higher 
security level increases, which makes the problem inefficient for practical cryptographic applications.
The Ring Learning with Errors problem \cite{lyubashevsky2010ideal} was introduced by Vladimir Lyubashevsky as an optimization of the initial LWE problem.

% \newpage
\subsection{Ring Learning with Errors}
Lyubashevsky's solution \cite{lyubashevsky2010ideal} suggests a more lightweight and secure approach in implementing encryption algorithms by applying the ring structure over a finite field of moduli, onto the polynomial representation 
of public and private keys. With that being said, using polynomials within a ring structure instead of vectors and matrices should be more efficient storage-wise, leading to space complexities of $O(n)$.\\
% However, if we were to plainly multiply each polynomial with another, we would end up doing modulo multiplication of all coefficients with each other. When combining this method with the large parameters passed to the RLWE
% problem, the computations become highly convoluted. From a security perspective, this is beneficial, particularly in the context of FHE and PQC, which both make use of the ring property and the polynomials in afferrent applications. 
% However, the issue lies within the actual computation in the technique. For this reason, more advanced modular multiplication methods have been introduced, such as Montgomery Reduction or Barrett Reduction.

Assuming that there exists some repeating "pattern'' in the LWE samples, we deduce that: if we consider $d$ to be a power of $2$, and if we assume that the $\mathbf{a}$ vectors arrive in blocks of $d$ samples
$\mathbf{a}_1, \mathbf{a}_2,..., \mathbf{a}_d \in \mathbb{Z}^d_q$, where $\mathbf{a}_1 = a_1, a_2,..., a_d$ is still chosen uniformly at random , we can conclude that the remaining vectors $\mathbf{a}_i$
can be reproduced as ($a_i,..., a_n, -a_1,..,-a_{i-1}$). This notation resembles a ring structure, where we write the rest of the $\mathbf{a}_i$ vectors in terms of $\mathbf{a}_1$, which reduces the 
complexity to $O(n)$ elements of $\mathbb{Z}_q$ as opposed to $O(n^2)$ for Learing with Errors.

Mathematically, this would imply trading the group of integers modulo $q$, $\mathbb{Z}^d_q$, for the ring of all polynomials over a finite field of moduli, $\mathbb{Z}_q / \langle x^d + 1\rangle$, which are all then reduced by $x^d + 1$
to ensure that the polynomial degree does not exceed $d-1$ (this needs to go into the notation section).
The additional requirements for this change  to take place is having the dimension $d$ be a power of two, which further guarantees that $x^d + 1$ is irreducible over the rationals in order to provide
a quotient ring structure.

With that being said, the \gls{rlwe} equivalent of the original LWE problem would be:
\begin{equation*}
  a^{(i)}(x) \cdot s(x) + e^{(i)}(x) \approx b^{(i)}(x) \pmod{x^d + 1, q}
  \label{The Ring-Learning with Errors Problem}
\end{equation*}


\paragraph{Hardness.} Solving the \gls{rlwe} problem implies a solution to the worst-case lattice problems, which are restricted to the family of $ideal\ lattices$ \cite{lyubashevsky2010ideal}. Ideal lattices are mathematical
structures that satisfy a certain symmetry condition: if ($a_1, a_2,..., a_d$) is a lattice vector, then so is the ring representation of it ($x_2,...,x_n, -x_1$).
Therefore, we can conclude that no changes over the hardness would occur in the context of Ring-Learning with Errors.
% \subsection{Polynomial multiplication}
\subsection{Polynomial Multiplication and Modular Reduction}
% \texttt{(Here I am not entirely sure how much in depth I should go, but I think that the description below does not suffice).}\\
Ring-Learning with Errors brings optimizations to Learning with Errors only space-wise through the ring structure and polynomial representation.
However, multiplying polynomials still yields $O(n^{2})$ time complexity in case of naive polynomial multiplication. This approach will do modular
multiplication of all polynomial coefficients with each other.
To reduce the complexity to quasi-linear time, Number Theoretic Transform (NTT) is applied and thus, the time complexity drops to $O(nlogn)$.
Polynomial multiplication needs two types of multiplication:
\begin{enumerate}
  \item Multiplication between polynomial coefficients and \textit{twiddle factors}.
  \item Element-wise multiplication between polynomial coefficients
\end{enumerate}
In both cases, modular reduction techniques are applied. However, since modular reduction algorithms such as Montgomery and Barrett require precomputation steps, it is crucial to find reduction optimization methods
in order to streamline the NTT-step. If this transformation is done efficiently, the performance of polynomial multiplication will increase significantly, and resources such as area, power and time are therefore preserved.
% Efficient modular multiplication and reduction are at the heart of many R-LWE cryptographic applications, which implicitly use polynomial multiplication due to the ring representation.
% If we recall the R-LWE problem formula from the previous chapter
% we can observe three main operations that take place:
% \begin{enumerate}
%   \item Polynomial multiplication between polynomials and their coefficients
%   \item Modular multiplication, which involves multiplication between two terms that are then reduced by a modulus.\\
%         In the case of R-LWE , the multiplication takes place between polynomials and the result is reduced by $x^n + 1$.
%   \item Modular reduction, which can be easily observed in R-LWE, is done firstly to reduce the polynomial degree of the polynomial multiplication result by $x^n + 1$,
%   then, the polynomial coefficients are reduced modulo $q$ to ensure they lie within a certain range.
% \end{enumerate}

% To streamline cryptographic schemes, it is essential that these operations are performed efficiently. One of the existing drawbacks is the modulo operation, which, 
% when performed using traditional long division, is highly inefficient.


\subsection{Montgomery Reduction}
Montgomery Reduction \cite{montgomery1985modular} is an efficient method that performs modular reduction after integer multiplication ($x \cdot y\ mod\ Q$).
Its efficiency comes from replacing traditional long division with simple bitwise operations, as well as from precomputing the input values into the ``Montgomery form''.
% The Montgomery form represents the input by using residue classes without affecting modular addition or subtraction. 

\paragraph{Montgomery Form.} We firstly define a modulus $Q > 1$ such that $\{x \in \mathbb{Z}\ |\ x \equiv r\ mod\ Q\}$. Then we select a radix (number base) $R = 2^n$, where 
$n = machine\ word\ size$, and $gcd(Q, R) = 1$, with $R > Q$. This choice of $R$ ensures that the computations are inexpensive by making the shifting operation possible.
If $x$ is an integer, its Montgomery form is:
\begin{equation*}
  xR \mod Q. 
\end{equation*}

This conversion should have no impact over addition or subtraction, because the result format in Montgomery
form is 1:1 as its integer counterpart due to the distributive law.
However, this is not the case for integer multiplication. Let $x,\ y$ be integers in the range $[0,\ Q-1]$. The Montgomery form of $x\cdot y\ mod\ Q$ is:

\begin{equation*}
  (xR\ mod\ Q)(yR\ mod\ Q)\ mod\ Q\ =\ (xyR)R\ mod\ Q.
\end{equation*}

We notice that after the multiplication, the result contains an extra $R$ that needs to be eliminated. A simple method to achieve this would be division by $R$, but
the result is not divisible by $R$ due to the modulo operation. In this case, we can employ the modular inverse of $R$, which is $R^{-1}$ with the condition that $RR^{-1} - QQ' = 1$,
and $Q'$ is the set of integers satisfying  $0 < R^{-1} < Q$ and $0 < Q' < R$.
By introducing the inverse, $R^{-1}$, the additional $R$ is reduced as follows:

\begin{equation*}
  (xR\ mod\ Q)(yR\ mod\ Q)R^{-1} \equiv (xR)(yR)R^{-1} \equiv (xyR)RR^{-1} \equiv (xy)R\ mod\ Q.
\end{equation*}
\\
Thus, we have converted the multiplication between $x, y$ in Montgomery form.

% \newpage
\paragraph{Reduction Algorithm.} 
Furthermore, we will discuss how to apply modular reduction to an integer in Montgomery form by outlining the reduction algorithm below.

\begin{algorithm}
  \caption{Montgomery Reduction (REDC)}
  \label{alg:REDC}
  \begin{algorithmic}[1]
  \REQUIRE $T, Q, R = 2^n$, where $\gcd(R, Q) = 1$.
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
  \ENSURE $TR^{-1} \mod Q$
  \STATE $m \leftarrow (T \mod R)Q' \mod R$
  \STATE $t \leftarrow (T + m \cdot Q) / R$
  \IF{$t \geq Q$}
      \STATE $t \leftarrow T - Q$
  \ENDIF
  \RETURN $t$
  \end{algorithmic}
\end{algorithm}

The input $T$ is any integer, or, in our case, the product between $(x,\ y)$ in Montgomery form. $Q$ is the modulus, $R=2^n$ is the Montgomery radix or base. The condition $gcd(Q, R) = 1$
ensures that Montgomery reduction can be applied, as it requires that $R$ has a modular inverse modulo $Q$. The integer $Q'$ can be computed similarly to $R'$, 
by using the Extended Euclidean Algorithm.

The intermediate value $m$ is computed to ensure that $T+mQ$ is divisible by $R$. Knowing that $Q \cdot Q' \equiv -1\ mod\ R$,
we can infer that $Q \cdot Q' + 1$ is a multiple of $R$. By applying $Q'$ to m, we ensure that it will "undo" $Q\ mod\ R$ from the computation of $t$,
such that $(T + m \cdot Q)\ mod\ R = 0$.
To validate this, we extend $T + mQ$ as follows:

\begin{equation*}
  T + mQ \equiv T + ((T\ mod\ R)Q'\mod R)Q \equiv T + T \cdot \underbrace{QQ'}_{\text{\( QQ' \equiv -1 \mod R \)}} \equiv T-T \equiv 0 \mod R
\end{equation*}

\paragraph{Hardware implementation.} Even though the REDC algorithm delivers the desired result in an optimal manner, implementing it in hardware could lead to a 
few issues, among which the most obvious one is the computation of $N'$. In this regard, we are also presenting an iterative method that uses addition, shifts and
subtractions.

Let $R=2^n$, $gcd(R, Q)=1$, let $Q$ be odd. The multiplicands are numbers in binary representation, $x, y$, where $x = (x_{n-1}, x_{n-2},...,x_{0})_2$ and $0 \leq y \leq Q$. 
We define an accumulator value whose parity bit determines whether or not the intermediate sum needs to be adjusted by adding $Q$ or not. In both cases, we divide the result by
$2$, or we shift right by $1$. After $n$ iterations, the attained result is $xyR^{-1} \mod Q$. \cite{montgomery1985modular}, \cite{7219961}

\begin{algorithm}
  \caption{Montgomery Reduction Hardware Implementation}
  \label{alg:montgomery}
  \begin{algorithmic}[1]
  \REQUIRE \( X, Y, Q \) and \( Q \) is odd, \( X, Y < Q \)
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
  \ENSURE \( Acc \leftarrow X \times Y \times R^{-1} \mod Q \)
  \STATE \( Acc \leftarrow 0 \)
  \FOR{ \( i = 0 \) \TO \( n-1 \)}
      \STATE \( Acc \leftarrow Acc + x_i \times Y \)
      \IF{ \( r_0 = 0 \) }
          \STATE \( Acc \leftarrow Acc / 2 \)
      \ELSE
          \STATE \( Acc \leftarrow (Acc + Q) / 2 \)
      \ENDIF
  \ENDFOR
  \RETURN \( Acc \)
  \end{algorithmic}
\end{algorithm}


% For addition and subtraction, the result finds itself within the ranges $[0, 2Q-2]$, $[-Q+1, Q-1]$ respectively. If the sum exceeds $Q-1$ on the positive
% side or $0$ on the negative side, we can map the result back to $[0, Q-1]$ by either subtracting or adding $Q$.

% However, this is not the case with multiplication, where the result lies within the range $[0, Q^2 - 2Q + 1]$. To reduce the result into the range $[0, Q-1]$, we need to perform division,
% which the bottleneck in many hardware applications. By using the Montgomery form, we ensure that long division is avoided by representing the inputs as multiples of $R$.
\newpage
\subsection{Barrett Reduction}
Barrett's reduction \cite{barrett1986implementing} started out as a technique provided by Paul Barrett in 1986 to obtain a high-speed implementation of the RSA encryption algorithm
on a digital signal processing chip. This technique achieved a two and a half second encryption time on average for $512$ bit exponent and modulus, on a first generation digital signal processor (DSP).
A digital signal processor is a multiplier accumulator (MAC) and a fast microprocessor that facilitates fast multiplication due to the multiplier unit it includes in addition to the arithmetic logic unit (ALU).

The algorithm is known for its efficiency due to replacing division by cheaper multiplication and by precomputing terms while using bitshift operations.
Required are a \textbf{fixed} modulus $M$ and a radix $R = 2^{2k}$, where $k$ is the length of the modulus. The radix and the modulus play a crucial role in the precomputation step.
In addition, the input multiplicands, $x, y$ must be elements of $[0, M-1]$.

Taking into consideration all previous input constraints, we seek to retrieve the remainder of the operation $(x \cdot y \mod M)$.
The schoolbook method presented below \cite{ModArith} first computes the quotient $q$, which is further used to compute the remainder $r$.
However, this method is not suitable for hardware implementation due to the division operation which is very expensive.
\begin{align*}
  &t = x \cdot y\\
  &q = \lfloor t / m \rfloor\\
  &r = t - q \cdot m.
\end{align*}
In this regard, Barrett's algorithm introduces the precomputation step where the floating point term $\frac{1}{m}$ is approximated by truncating the least $2k$ bits found after $(.)$,
and converted to an integer by a shift to the left by the radix $R$. The approximation step is shown in \hyperref[fig:ApproximationStepBarrett]{Figure~\ref{fig:ApproximationStepBarrett}}.
% \textbf{Modulus} $m = 7069$ \textbf{is 13 bits long. Hence} $k = 13$.\\
% \begin{align*}
% \frac{1}{m} & = 0.00014146272457207525816947234_{10} \\
%             & = 0.000000000000010010100010101011000111\ldots_{2} \\
%             & \approx \textcolor{blue}{0.000000000000010010100010101}_{2} 
%               \quad \textcolor{blue}{\text{Truncate least } 2k=26 \text{ bits}}
% \end{align*}
% % \]

% % \[
% % 0.000141456723213958_{10}
% % \]

% % \[
% \begin{align*}
% \mu & = \textcolor{blue}{0.000000000000010010100010101_{2}} \ll 26 
%       \quad \text{multiply by } 2^{26}\\
%     & = 10010100010101_{2} \\
%     & = 9493_{10}
% \end{align*}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/mu-approximation-step.png}
  \caption{Approximation step in Barrett's reduction \cite{ModArith}}
  \label{fig:ApproximationStepBarrett}
\end{figure}
\newpage
The value of the precomputed term will not change as long as the value of the modulus remains constant \cite{4272869}.
Direct division is as well avoided in the approximation step of the quotient (line 3 in the algorithm) through the division by the radix, which represents a shift to the right in hardware implementations.

\begin{algorithm}
  \caption{Barrett Reduction}
  \label{alg:montgomery}
  \begin{algorithmic}[1]
  \REQUIRE \( X, Y, M \), \( X, Y < M \)
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
  \ENSURE \(X \cdot Y \mod M \)
  \STATE \( T \leftarrow X \cdot Y \)
  \STATE \( \mu \leftarrow \lfloor(1 / M) \cdot 2^{2k}\rfloor \)
  \STATE \( q \leftarrow \lfloor (T \cdot \mu) / 2^{2k} \rfloor \)
  \STATE \( r \leftarrow T - q \cdot M \)
  \IF{ \( r \geq M \) }
      \STATE \( r \leftarrow r - M \)
  \ENDIF
  \RETURN \( r \)
  \end{algorithmic}
\end{algorithm}
% \newpage
\paragraph{Mathematically,} the formula for finding the remainder of $(x \cdot y \mod M)$ is given by:
\begin{align*}
  &r = T - \left\lfloor \frac{T \cdot \lfloor \frac{2^{2k}}{m}\rfloor}{2^{2k}} \right\rfloor \cdot M
\end{align*}
Which is further equal to:
\begin{align*}
  &r = T - \left\lfloor \frac{T \cdot \mu}{R} \right\rfloor \cdot M
\end{align*}
Where $T = x \cdot y$ and R is the radix.

% \newpage
\subsection{Shift \& Add}
%  \texttt{(Shift and Add should be common knowledge, and I'm also not really discussing anything related to it in the introduction
%  since this method is intuitively applied in hardware algorithms)}\\
Shift \& Add is a classic multiplication method which is mainly employed in in the context of hardware implementation.
This method is at the heart of many algorithms, ranging from cryptographic applications to graphics and image processing due to the lightweight
approach it has over the multiplication. The main idea stands in leveraging bitshift operations which are equivalent to either multiplication or division.
Therefore, given two integers $x, y$ in binary representation, the algorithm is stated as follows:

\begin{algorithm}
  \caption{Shift \& Add multiplication}
  \label{alg:montgomery}
  \begin{algorithmic}[1]
  \REQUIRE \( X, Y\ \text{in binary representation}\)
  \renewcommand{\algorithmicensure}{\textbf{Output:}}
  \ENSURE \(X \cdot Y\)
  \STATE \( Prod \leftarrow 0\ (\text{because it is computed as a sum}) \)
  \FOR{ \( i = bitlength(X) - 1 \) \TO \( 0 \)}
      \IF{ \( X[i] = 1 \) }
          \STATE \( Prod = Prod + X[i] << i \)
      \ENDIF
  \ENDFOR
  \RETURN \( Prod \)
  \end{algorithmic}
\end{algorithm}

We start from the least significant bit of either of the numbers, in our case $x$. If the current bit is $1$, we shift the value to the left by $i$ and
we add it to the $Prod$ variable. Otherwise, if the bit value is 0, nothing is done. At the end, the value of the product is returned.

Nonetheless, this algorithm is not only well-suited for cutting down the effort of multiplication. In \cite{sreehari2012application}, the reduction step speed 
is increased significantly when using shifts and additions in combination with Mersenne/Pseudo-Mersenne primes for moduli ($M_p = 2^p - 1$). This improvement 
stems from the fact that Mersenne primes are numbers that are one less than a power of two, and powers of two are particularly efficient for bitshift operations. 
Thus, this special kind of modulus is a good candidate for fast modular multiplication implementation on FPGA or ASIC.
%--- CONCLUSION ----------------------------------------------------------------
% \newpage
\section{Conclusion}
\label{sec:conclusion}
In this paper, we explore different implementations and designs of the Montgomery and Barrett modular multipliers, on FPGA and ASIC, for lattice-based cryptosystems.
From \cite{kong2006comparison} we learn that separated and interleaved designs of the Montgomery algorithm yield the same results on an FPGA as the word size increases.
Additionally, for word lengths up to 16 bits, Barrett's reduction with a precomputed inverse of the modulus performs better. In \cite{sreehari2012application}, the speed 
of long-integer modular multiplication is improved by replacing the reduction step in a modified Barrett algorithm with shifts and additions, leveraging the properties of 
Mersenne and Pseudo-Mersenne primes.

For FHE ASIC implementations, bit parallel designs are more efficient than bit-serial implementations in terms of area- and power consumption. A Montgomery multiplier 
with a constrained modulus yields the best results area- and power-wise by limiting the range of possible modulus values.
Under the circumstances of post-quantum cryptographic schemes, the Number Theoretic Transform is optimized using algorithms that employ vector integer instructions in
their implementation. This detail facilitates parallelization and pipelining, which streamline the modular reduction process.
Finally, we can infer that for both FPGA and ASIC platforms, parallel designs for modular multipliers manage to preserve the most resources.

% Provide the conclusions of your short report (max. $\frac{1}{2}$ page).

% This is structured similarly to the abstract and introduction.
% However, unlike the abstract, it can be partially written in past tense (for actions you performed and results that you found) as well as future tense or conditional (for predictions what the impact of your work will be).
% Start by briefly recalling the main motivation and main goal of your work.
% Repeat the main hard facts, performance numbers, and properties of your solution/work.
% %
% Emphasize your main insights, findings, and lessons learned.
% If you do not have a dedicated discussion section, you can discuss your findings and put them into perspective.
% If you have any recommendations based on your work, phrase them here.

% Finally, you can point out open problems that call for future work, but phrased in a positive way -- as opportunities.
% There should be no complete surprises (such as significant shortcomings not discussed earlier), but you can provide some new thoughts and ideas.
%--- ACRONYMS --------------------------------------------------------------
% \newglossaryentry{lwe}{
%     name={Learning with Errors},
%     description={Learning with Errors}
% }
% \newacronym{lwe}{LWE}{Learning with Errors}

% \newglossaryentry{rlwe}{
%     name={Ring Learning with Errors},
%     description={Ring-Learning with Errors}
% }
% \newacronym{rlwe}{R-LWE}{Ring Learning with Errors}


% \glsaddall
% \printglossary
%--- BIBLIOGRAPHY --------------------------------------------------------------
\newpage
\printbibliography[heading=subbibliography]

% \input{abbreviations.tex}

\end{document}
